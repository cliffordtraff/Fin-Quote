<!-- Generated by SpecStory, Markdown v2.0.0 -->

<!-- cursor Session 216d4de0-f054-4fac-be2a-f91882fc56ba (2025-11-07 19:04Z) -->

# Does playwright mcp consume many tokens? (2025-11-07 19:04Z)

_**User (2025-11-07 01:57Z)**_

does playwright mcp use a lot of tokens

---

_**Agent (model gpt-5, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Estimating token consumption**

I'm thinking about what consumes tokens in various processes. DOM snapshots, network logs, console logs, performance audits, and full-page screenshots are all factors. 

When it comes to typical usage estimates, DOM snapshots range from about 50 to 300 KB, translating to roughly 2,000 to 12,000 tokens. Console logs are small, but network logs can be extensive if there are many requests. Screenshots depend on resolution, so I need to be cautious about how I present this. Providing rough ranges is useful without making precise claims.
**Providing concise tips**

To reduce token usage effectively, I can share some actionable tips. For instance: 

- Use targeted selectors.
- Wipe logs before capturing.
- Wait for specific text before capturing.
- Capture element-only screenshots.
- Throttle snapshot frequency and limit to errors.
- Use pagination and compress/limit logs.

When deciding between a screenshot and a snapshot, use screenshots for visual layout bugs and snapshots for content. Focus on individual network requests for API issues and use getConsoleErrors for console errors. 

Starting small is key, and I’ll keep it clear and jargon-free!</details></think>

---

### Short answer
Not inherently. Playwright MCP itself doesn’t “use tokens”; tokens are spent on whatever data it returns to the model. If you request big payloads (full DOM snapshots, all network logs, full-page screenshots), token usage can spike.

### What drives token use
- **Large text blobs**: Full-page DOM/accessibility snapshots and long logs can be thousands of tokens.
- **Network logs**: Dumping all requests/responses is often the biggest source of tokens.
- **Screenshots**: Images count as “vision tokens” and scale with resolution/quantity.

### Practical guidelines to keep it low
- **Be specific**: Query a single element or small region, not the whole page.
- **Filter logs**: Grab only errors or a narrow time window; clear logs first, then reproduce and capture.
- **Avoid full-page dumps**: Prefer targeted snapshots over full DOM; capture one problematic request instead of all.
- **Use smaller images**: Element or viewport screenshots over full-page; only when visuals are required.
- **Don’t repeat**: Reuse prior info; capture once unless something changed.

### Rule of thumb
Start with the smallest, most targeted capture that answers your question; scale up only if needed. This keeps both latency and token costs down.

---

_**User (2025-11-07 01:57Z)**_

but it keeps saying 'large mcp response' as i am using it

---

_**Agent (model gpt-5, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Providing guidance on MCP warnings**

The user is experiencing Large MCP response warnings around 10k tokens when using Playwright. To help reduce these responses, I can suggest several actionable steps. First, using `onlyErrors: true` for console logs, filtering network logs, and limiting snapshots could help. The user can also avoid capturing the full DOM for navigate actions by adjusting settings to set `verbose: false` or disabling logging altogether. These adjustments should help minimize response size and better manage token use in their project.
**Adapting tool usage for efficiency**

I need to help the user with the large JSON outputs from their tool usage. While we can’t change how tools output, we can suggest alternatives. For instance, instead of logs, using "browser_snapshot" directly can still produce large sizes, so it might be better to opt for viewport screenshots or specific element screenshots. They can also limit console messages using `getConsoleMessages(onlyErrors: true)` for a streamlined view. Cleaning logs before navigation and reusing pages can minimize sizes too. Finally, using 'browser_tools' by adjusting the order of actions might help.
**Optimizing tool usage for smaller outputs**

I need to be cautious about calling snapshots only when necessary. For screenshots, I should use 'fullPage: false' to minimize size and suggest a 'type: jpeg' option to further reduce it. When it comes to network logs, using the tool to fetch only the last N logs or filtering them can help; specifically, I can focus on errors with `mcp_browsertools_getNetworkErrors`. The warnings stem from executing "Navigate to URL," which captures extensive data including the entire HTML. We need to find ways to avoid this while ensuring we keep the outputs manageable.
**Proposing usage adjustments for MCP**

I’m considering the option to turn off "Return page html" or "auto-snapshot" in the MCP configuration. However, as the user, changing server settings isn’t straightforward unless it’s done within the plugin code. I can suggest avoiding the page snapshot tool after navigation, although there’s already a warning when navigating. The logs indicate that the 'navigate' tool returns page content by default. To reduce this, I could recommend adjusting 'logLevel' options, but I can’t modify parameters for navigate, which complicates things.
**Providing steps to reduce MCP warnings**

To address the user's concern about large MCP response warnings, I can suggest practical steps. First, prefer using BrowserTools for navigation instead of Playwright MCP, as it generates less verbose content. Second, avoid "browser_snapshot" unless selecting specific elements; instead, use "take screenshot." For logs, stick to `getConsoleErrors` and `getNetworkErrors`, and remember to wipe logs beforehand. Also, try to minimize actions per navigation by keeping the page open and consider collapsing tool output settings. If warnings persist, it's normal, but these steps should help reduce token usage significantly.</details></think>

---

Short answer: The warning means the tool is returning big payloads (HTML dumps, long logs, or images). It’s normal, but it can burn tokens fast.

Do this to stop the “large MCP response” warnings:
- Use lighter browser tools, not Playwright, when possible:
  - Prefer `browser - Navigate` over `playwright - Navigate` for simple page loads.
  - Avoid `browser_snapshot` unless you truly need the DOM tree; it’s huge.
- Limit logs:
  - Use `getConsoleErrors` and `getNetworkErrors` instead of full logs.
  - Call `wipeLogs` right before you reproduce the issue so captures are short.
- Keep visuals small:
  - Take a viewport or element screenshot (not full-page); only when needed.
  - Skip screenshots if console/network errors already answer the question.
- Reduce repeats:
  - Reuse the same page/session instead of navigating repeatedly.
  - Don’t capture the same data multiple times; capture once and proceed.

Rule of thumb: Avoid DOM snapshots and “all logs.” Stick to errors-only + small screenshots. This typically keeps responses under a few hundred tokens. Want me to show a minimal, low-token debug flow for your `/admin/evaluations` page?

---

_**User (2025-11-07 01:57Z)**_

how are we using meta-prompting with this codebase?

---

_**Agent (model gpt-5, mode Agent)**_

<think><details><summary>Thought Process</summary>
**Updating tool usage process**

I think we can skip the todo for this task, but I need to provide a status update when tool calls are made. Since I plan to read files to generate code references, I'll include a short status update before these calls. I should utilize `multi_tool_use.parallel` to read from `lib/tools.ts` and `app/actions/ask-question.ts` at the same time. I’m considering adding `lib/llm-judge.ts` too, even if it’s untracked, since reading might still be valid. Let’s focus on running the file reads in parallel!</details></think>

---

I'll quickly scan the prompt-building code to point to the exact places where the meta-prompt (static, reusable instructions) is defined and used. Then I'll explain how caching/meta-prompting works in this app with short code references.

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/cliffordtraff/Desktop/Fin Quote/lib/tools.ts</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file">
<details><summary>Tool use: **read_file** • Read file: /Users/cliffordtraff/Desktop/Fin Quote/app/actions/ask-question.ts</summary>

</details>
</tool-use>

---



---

I scanned `lib/tools.ts` and `app/actions/ask-question.ts` to show exactly where the meta-prompt lives and how it’s used.

### What “meta‑prompting” means here
- You keep a stable, reusable instruction block (the “router policy”) that tells the model how to pick a tool and how to format output. That’s the meta‑prompt.
- You reuse it across queries and layer per‑question details on top.

### Where it is
- Tool selection meta‑prompt (the router policy) is a long, static instruction block:

```66:75:/Users/cliffordtraff/Desktop/Fin Quote/lib/tools.ts
// Static instructions that will be cached by OpenAI
const TOOL_SELECTION_STATIC_PROMPT = `You are a router. Choose exactly one tool and return ONLY valid JSON: {"tool": string, "args": object}. No prose.

Available Tools:
```

- There’s also a structured version set up for caching (not currently wired in):

```331:341:/Users/cliffordtraff/Desktop/Fin Quote/lib/tools.ts
// New function that returns structured messages for caching
export const buildToolSelectionMessages = (userQuestion: string) => [
  {
    role: 'system' as const,
    content: TOOL_SELECTION_STATIC_PROMPT
  },
  {
    role: 'user' as const,
    content: `User question: "${userQuestion}"`
  }
]
```

### How it’s used at runtime
- The app currently uses the string version (not the cached/messages variant) and adds conversation history + the user question:

```257:277:/Users/cliffordtraff/Desktop/Fin Quote/app/actions/ask-question.ts
// Step 1: Tool selection with conversation history
const toolSelectionStart = Date.now()
const selectionPrompt = buildToolSelectionPrompt(userQuestion)

// Build messages array with conversation history
const selectionMessages: SimpleMessage[] = [
  { role: 'system', content:
    'You are Fin Quote routing assistant. Your ONLY job is to pick exactly one tool...' },
  ...conversationHistory.slice(-10).map(...),
  { role: 'user', content: selectionPrompt },
]
```

- For the final answer, you combine rules + dynamic facts in one prompt (so it’s not split for caching). This big instruction block functions as a second meta‑prompt for answer formatting/validation:

```374:384:/Users/cliffordtraff/Desktop/Fin Quote/lib/tools.ts
export const buildFinalAnswerPrompt = (
  userQuestion: string,
  factsJson: string
) => `You are an analyst. Answer the user using ONLY the provided facts.

User question: "${userQuestion}"

Facts (JSON rows):
${factsJson}

CRITICAL VALIDATION RULES - Follow These Exactly:
`
```

- A smaller system-level formatter is also layered on top:

```532:539:/Users/cliffordtraff/Desktop/Fin Quote/app/actions/ask-question.ts
const formattingInstructions = [
  'You are Fin Quote analyst assistant.',
  'Use only the provided facts; never guess...',
  'Respond in plain text sentences with no Markdown...',
  ...
].join(' ')
```

```543:555:/Users/cliffordtraff/Desktop/Fin Quote/app/actions/ask-question.ts
const answerMessages: SimpleMessage[] = [
  { role: 'system', content: formattingInstructions },
  ...recentHistory.map(...),
  { role: 'user', content: answerPrompt },
]
```

### Net: how you’re using meta‑prompting
- Tool routing uses a single, reusable meta‑prompt (router policy) plus conversation history and the current question.
- Answer generation uses a large, reusable rules block (validation/formatting policy) combined with dynamic facts; plus a small system formatter.
- A cache‑ready messages version exists for tool selection but isn’t hooked up; answer rules aren’t split into “static cached” + “dynamic” yet.

If you want, I can refactor selection to use the cache‑ready messages and split the answer rules into static cached + dynamic facts to reduce tokens.

---

